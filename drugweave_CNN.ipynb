{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-01T10:28:59.255328Z",
     "iopub.status.busy": "2025-01-01T10:28:59.254857Z",
     "iopub.status.idle": "2025-01-01T10:28:59.618942Z",
     "shell.execute_reply": "2025-01-01T10:28:59.618058Z",
     "shell.execute_reply.started": "2025-01-01T10:28:59.255286Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/davis-and-kiba/kiba.txt\n",
      "/kaggle/input/davis-and-kiba/davis.txt\n",
      "/kaggle/input/davis-and-kiba/davis-filter.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Processing the Data and encoding sequence for MLP** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:28:59.620650Z",
     "iopub.status.busy": "2025-01-01T10:28:59.620172Z",
     "iopub.status.idle": "2025-01-01T10:28:59.624498Z",
     "shell.execute_reply": "2025-01-01T10:28:59.623648Z",
     "shell.execute_reply.started": "2025-01-01T10:28:59.620605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "davis_filter_dir = '/kaggle/input/davis-and-kiba/davis-filter.txt'\n",
    "kiba_dir = '/kaggle/input/davis-and-kiba/kiba.txt'\n",
    "davis_dir = '/kaggle/input/davis-and-kiba/davis.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:28:59.626436Z",
     "iopub.status.busy": "2025-01-01T10:28:59.626193Z",
     "iopub.status.idle": "2025-01-01T10:29:00.125799Z",
     "shell.execute_reply": "2025-01-01T10:29:00.124891Z",
     "shell.execute_reply.started": "2025-01-01T10:28:59.626414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "col_name = ['drug_id', 'prot_id', 'SMILES', 'sequence amino acid', 'sequence amino acid affinity']\n",
    "dd = pd.read_csv(davis_dir, sep=' ', header=None)  \n",
    "dd.columns = col_name\n",
    "dd = pd.DataFrame(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:00.128088Z",
     "iopub.status.busy": "2025-01-01T10:29:00.127617Z",
     "iopub.status.idle": "2025-01-01T10:29:00.281161Z",
     "shell.execute_reply": "2025-01-01T10:29:00.280401Z",
     "shell.execute_reply.started": "2025-01-01T10:29:00.128045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ddf = pd.read_csv(davis_filter_dir, sep=' ', header=None)  # TODO modify the dataset_dir\n",
    "ddf.columns = col_name\n",
    "ddf = pd.DataFrame(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:00.282378Z",
     "iopub.status.busy": "2025-01-01T10:29:00.282080Z",
     "iopub.status.idle": "2025-01-01T10:29:01.811972Z",
     "shell.execute_reply": "2025-01-01T10:29:01.811016Z",
     "shell.execute_reply.started": "2025-01-01T10:29:00.282342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dk = pd.read_csv(kiba_dir, sep=' ', header=None)  # TODO modify the dataset_dir\n",
    "dk.columns = col_name\n",
    "dk = pd.DataFrame(dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:01.813414Z",
     "iopub.status.busy": "2025-01-01T10:29:01.813062Z",
     "iopub.status.idle": "2025-01-01T10:29:01.819853Z",
     "shell.execute_reply": "2025-01-01T10:29:01.818904Z",
     "shell.execute_reply.started": "2025-01-01T10:29:01.813382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davis_shape: (30056, 5)\n",
      "Davis_filter_shape: (9125, 5)\n",
      "kiba_shape: (118254, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Davis_shape:\", dd.shape)\n",
    "print(\"Davis_filter_shape:\", ddf.shape)\n",
    "print(\"kiba_shape:\", dk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:01.821150Z",
     "iopub.status.busy": "2025-01-01T10:29:01.820878Z",
     "iopub.status.idle": "2025-01-01T10:29:01.834568Z",
     "shell.execute_reply": "2025-01-01T10:29:01.833625Z",
     "shell.execute_reply.started": "2025-01-01T10:29:01.821122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "amino_acids = \"ARNDCEQGHILKMFPSTWYV\"  # 20 standard amino acids\n",
    "amino_acid_to_number = {aa: idx + 1 for idx, aa in enumerate(amino_acids)}\n",
    "def encode_sequence(sequence):\n",
    "    return [amino_acid_to_number[aa] for aa in sequence if aa in amino_acid_to_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:01.835875Z",
     "iopub.status.busy": "2025-01-01T10:29:01.835574Z",
     "iopub.status.idle": "2025-01-01T10:29:14.879104Z",
     "shell.execute_reply": "2025-01-01T10:29:14.877990Z",
     "shell.execute_reply.started": "2025-01-01T10:29:01.835853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dd['encoded_sequence'] = dd['sequence amino acid'].apply(encode_sequence)\n",
    "ddf['encoded_sequence'] = ddf['sequence amino acid'].apply(encode_sequence)\n",
    "dk['encoded_sequence'] = dk['sequence amino acid'].apply(encode_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:56:30.599097Z",
     "iopub.status.busy": "2025-01-01T10:56:30.598574Z",
     "iopub.status.idle": "2025-01-01T10:56:30.606998Z",
     "shell.execute_reply": "2025-01-01T10:56:30.605762Z",
     "shell.execute_reply.started": "2025-01-01T10:56:30.599039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def h(m):\n",
    "    if m > 0:\n",
    "        return 1\n",
    "    elif m == 0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def ConcordanceIndex(Affinities,Predictions):\n",
    "    Z = 0 \n",
    "    concordant_pairs = 0\n",
    "    \n",
    "    n = len(Affinities)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if Affinities[i] > Affinities[j]:\n",
    "                Z += 1\n",
    "                concordant_pairs += h(Predictions[i] - Predictions[j])\n",
    "    \n",
    "    if Z > 0:\n",
    "        return concordant_pairs / Z\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:14.892294Z",
     "iopub.status.busy": "2025-01-01T10:29:14.891887Z",
     "iopub.status.idle": "2025-01-01T10:29:14.909201Z",
     "shell.execute_reply": "2025-01-01T10:29:14.908250Z",
     "shell.execute_reply.started": "2025-01-01T10:29:14.892255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(Affinities,Predictions):\n",
    "    errors = np.array(Affinities) - np.array(Predictions)\n",
    "    return np.mean(errors**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:14.910573Z",
     "iopub.status.busy": "2025-01-01T10:29:14.910288Z",
     "iopub.status.idle": "2025-01-01T10:29:16.260839Z",
     "shell.execute_reply": "2025-01-01T10:29:16.260022Z",
     "shell.execute_reply.started": "2025-01-01T10:29:14.910548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def R2(Affinities, Predictions):\n",
    "    Affinities = np.array(Affinities)\n",
    "    Predictions = np.array(Predictions)\n",
    "    \n",
    "    SST = np.sum((Affinities - np.mean(Affinities))**2)\n",
    "    SSR = np.sum((Affinities - Predictions)**2)\n",
    "    \n",
    "    return 1 - (SSR/SST)\n",
    "\n",
    "def R02(Affinities, Predictions):\n",
    "    Affinities = np.array(Affinities).reshape(-1, 1)\n",
    "    Predictions = np.array(Predictions).reshape(-1, 1)\n",
    "    \n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(Predictions, Affinities)\n",
    "    Predictions = model.predict(Predictions)\n",
    "    \n",
    "    SST = np.sum((Affinities - np.mean(true_values))**2)\n",
    "    SSR = np.sum((Affinities - predictions)**2)\n",
    "    \n",
    "    return 1 - (SSR/SST)\n",
    "\n",
    "def RMSsquared(Affinities, Predictions):\n",
    "    Rsquared = R2(Affinities, Predictions)\n",
    "    R0squared = R02(Affinities, Predictions)\n",
    "    \n",
    "    return Rsquared * (1 - np.sqrt(Rsquared - R0squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:16.262222Z",
     "iopub.status.busy": "2025-01-01T10:29:16.261824Z",
     "iopub.status.idle": "2025-01-01T10:29:16.267507Z",
     "shell.execute_reply": "2025-01-01T10:29:16.266614Z",
     "shell.execute_reply.started": "2025-01-01T10:29:16.262196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Pearson_correlation(Affinities, Predictions):\n",
    "    Affinities = np.array(Affinities)\n",
    "    Predictions = np.array(Predictions)\n",
    "    \n",
    "    TrueAvg = np.mean(Affinities)\n",
    "    PredAvg = np.mean(Predictions)\n",
    "    \n",
    "    Num = np.sum((Affinities - TrueAvg) * (Predictions - PredAvg))\n",
    "    Den = np.sqrt(np.sum((Affinities - TrueAvg)**2) * np.sum((Predictions - PredAvg)**2))\n",
    "    \n",
    "    return Num/Den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:16.268829Z",
     "iopub.status.busy": "2025-01-01T10:29:16.268424Z",
     "iopub.status.idle": "2025-01-01T10:29:16.286764Z",
     "shell.execute_reply": "2025-01-01T10:29:16.285621Z",
     "shell.execute_reply.started": "2025-01-01T10:29:16.268791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def compute_aupr(Affinities, Predictions):\n",
    "    precision, recall, _ = precision_recall_curve(Affinities, Predictions)\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:55:03.544497Z",
     "iopub.status.busy": "2025-01-01T10:55:03.544140Z",
     "iopub.status.idle": "2025-01-01T10:55:03.552661Z",
     "shell.execute_reply": "2025-01-01T10:55:03.551750Z",
     "shell.execute_reply.started": "2025-01-01T10:55:03.544461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def backpropagation_two_hidden_layers(X, y, W1, b1, W2, b2, W3, b3, learning_rate):\n",
    "    \n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    y_pred = Z3 \n",
    "    \n",
    "    n = y.shape[0]\n",
    "    loss = np.mean((y - y_pred)**2)\n",
    "    dLoss_dZ3 = (2 / n) * (y_pred - y)\n",
    "    dLoss_dW3 = np.dot(A2.T, dLoss_dZ3)\n",
    "    dLoss_db3 = np.sum(dLoss_dZ3, axis=0, keepdims=True)\n",
    "    \n",
    "    dLoss_dA2 = np.dot(dLoss_dZ3, W3.T)\n",
    "    dLoss_dZ2 = dLoss_dA2 * sigmoid_derivative(Z2)\n",
    "    dLoss_dW2 = np.dot(A1.T, dLoss_dZ2)\n",
    "    dLoss_db2 = np.sum(dLoss_dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    dLoss_dA1 = np.dot(dLoss_dZ2, W2.T)\n",
    "    dLoss_dZ1 = dLoss_dA1 * sigmoid_derivative(Z1)\n",
    "    dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n",
    "    dLoss_db1 = np.sum(dLoss_dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    W1 -= learning_rate * dLoss_dW1\n",
    "    b1 -= learning_rate * dLoss_db1\n",
    "    W2 -= learning_rate * dLoss_dW2\n",
    "    b2 -= learning_rate * dLoss_db2\n",
    "    W3 -= learning_rate * dLoss_dW3\n",
    "    b3 -= learning_rate * dLoss_db3\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Training on Davis Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:29:16.305240Z",
     "iopub.status.busy": "2025-01-01T10:29:16.304898Z",
     "iopub.status.idle": "2025-01-01T10:29:16.338084Z",
     "shell.execute_reply": "2025-01-01T10:29:16.336829Z",
     "shell.execute_reply.started": "2025-01-01T10:29:16.305202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug_id</th>\n",
       "      <th>prot_id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>sequence amino acid</th>\n",
       "      <th>sequence amino acid affinity</th>\n",
       "      <th>encoded_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11314340</td>\n",
       "      <td>AAK1</td>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...</td>\n",
       "      <td>7.366532</td>\n",
       "      <td>[13, 12, 12, 14, 14, 4, 16, 2, 2, 6, 7, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11314340</td>\n",
       "      <td>ABL1(E255K)</td>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11314340</td>\n",
       "      <td>ABL1(F317I)</td>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11314340</td>\n",
       "      <td>ABL1(F317I)p</td>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11314340</td>\n",
       "      <td>ABL1(F317L)</td>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    drug_id       prot_id                                             SMILES  \\\n",
       "0  11314340          AAK1  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "1  11314340   ABL1(E255K)  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "2  11314340   ABL1(F317I)  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "3  11314340  ABL1(F317I)p  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "4  11314340   ABL1(F317L)  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "\n",
       "                                 sequence amino acid  \\\n",
       "0  MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...   \n",
       "1  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
       "2  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
       "3  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
       "4  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
       "\n",
       "   sequence amino acid affinity  \\\n",
       "0                      7.366532   \n",
       "1                      5.000000   \n",
       "2                      5.000000   \n",
       "3                      5.000000   \n",
       "4                      5.000000   \n",
       "\n",
       "                                    encoded_sequence  \n",
       "0  [13, 12, 12, 14, 14, 4, 16, 2, 2, 6, 7, 8, 8, ...  \n",
       "1  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  \n",
       "2  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  \n",
       "3  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  \n",
       "4  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:36:41.212663Z",
     "iopub.status.busy": "2025-01-01T10:36:41.212238Z",
     "iopub.status.idle": "2025-01-01T10:36:41.644047Z",
     "shell.execute_reply": "2025-01-01T10:36:41.642952Z",
     "shell.execute_reply.started": "2025-01-01T10:36:41.212616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_case_1(df):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for protein, group in df.groupby('prot_id'):\n",
    "        n_train = int(len(group) * 0.7)  # 70% for train\n",
    "        group = group.sample(frac=1, random_state=42)  # Shuffle the data\n",
    "        train_data.append(group.iloc[:n_train])\n",
    "        test_data.append(group.iloc[n_train:])\n",
    "    \n",
    "    train = pd.concat(train_data).reset_index(drop=True)\n",
    "    test = pd.concat(test_data).reset_index(drop=True)\n",
    "    return train, test\n",
    "\n",
    "def split_case_2(df):\n",
    "    unique_proteins = df['prot_id'].unique()\n",
    "    n_train_proteins = int(len(unique_proteins) * 0.9)  # 90% of proteins for train\n",
    "    train_proteins = np.random.choice(unique_proteins, size=n_train_proteins, replace=False)\n",
    "    \n",
    "    train = df[df['prot_id'].isin(train_proteins)]\n",
    "    test = df[~df['prot_id'].isin(train_proteins)]\n",
    "    return train, test\n",
    "\n",
    "def split_case_3(df):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # Group by drug and split each group\n",
    "    for drug, group in df.groupby('drug_id'):\n",
    "        n_train = int(len(group) * 0.7)  # 70% for train\n",
    "        group = group.sample(frac=1, random_state=42)  # Shuffle the data\n",
    "        train_data.append(group.iloc[:n_train])\n",
    "        test_data.append(group.iloc[n_train:])\n",
    "    \n",
    "    train = pd.concat(train_data).reset_index(drop=True)\n",
    "    test = pd.concat(test_data).reset_index(drop=True)\n",
    "    return train, test\n",
    "\n",
    "def split_case_4(df):\n",
    "    unique_drugs = df['drug_id'].unique()\n",
    "    n_train_drugs = int(len(unique_drugs) * 0.9)  # 90% of drugs for train\n",
    "    train_drugs = np.random.choice(unique_drugs, size=n_train_drugs, replace=False)\n",
    "    \n",
    "    train = df[df['drug_id'].isin(train_drugs)]\n",
    "    test = df[~df['drug_id'].isin(train_drugs)]\n",
    "    return train, test\n",
    "\n",
    "# Generate splits\n",
    "X1_train, X1_test = split_case_1(dd)  # Case 1: No new proteins in test\n",
    "X2_train, X2_test = split_case_2(dd)  # Case 2: New proteins in test\n",
    "X3_train, X3_test = split_case_3(dd)  # Case 3: No new drugs in test\n",
    "X4_train, X4_test = split_case_4(dd)  # Case 4: New drugs in test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T10:57:10.055775Z",
     "iopub.status.busy": "2025-01-01T10:57:10.055372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for Case 1: No new proteins in test\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 1/50\n",
      "Loss: 0.9911\n",
      "Training Metrics:\n",
      "MSE: 0.7647\n",
      "Pearson: 0.1900\n",
      "CI: 0.5810\n",
      "R2: 0.0274\n",
      "Test Metrics:\n",
      "MSE: 0.7535\n",
      "Pearson: 0.2373\n",
      "CI: 0.6267\n",
      "R2: 0.0416\n",
      "\n",
      "Epoch 2/50\n",
      "Loss: 0.9631\n",
      "Training Metrics:\n",
      "MSE: 0.7531\n",
      "Pearson: 0.2171\n",
      "CI: 0.5969\n",
      "R2: 0.0422\n",
      "Test Metrics:\n",
      "MSE: 0.7384\n",
      "Pearson: 0.2702\n",
      "CI: 0.6416\n",
      "R2: 0.0608\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def pad_sequences(sequences, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "    return [seq + [0] * (max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "def prepare_data(df):\n",
    "    # Pad sequences to same length\n",
    "    sequences = pad_sequences(df['encoded_sequence'].values)\n",
    "    X = np.array(sequences)\n",
    "    y = df['sequence amino acid affinity'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X = scaler_X.fit_transform(X)\n",
    "    y = scaler_y.fit_transform(y)\n",
    "    \n",
    "    return X, y, scaler_X, scaler_y\n",
    "\n",
    "def forward_pass(X, W1, b1, W2, b2, W3, b3):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    y_pred = Z3\n",
    "    \n",
    "    return y_pred, (A1, A2)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, scaler_y):\n",
    "    y_true_orig = scaler_y.inverse_transform(y_true)\n",
    "    y_pred_orig = scaler_y.inverse_transform(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true_orig, y_pred_orig)\n",
    "    pearson = Pearson_correlation(y_true_orig.flatten(), y_pred_orig.flatten())\n",
    "    ci = ConcordanceIndex(y_true_orig.flatten(), y_pred_orig.flatten())\n",
    "    r2 = R2(y_true_orig.flatten(), y_pred_orig.flatten())\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'Pearson': pearson,\n",
    "        'CI': ci,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, scaler_y, input_size=1000, hidden_size1=512, hidden_size2=256, output_size=1, \n",
    "                learning_rate=0.001, epochs=100, batch_size=32):\n",
    "    # Initialize weights\n",
    "    W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2. / input_size)\n",
    "    b1 = np.zeros((1, hidden_size1))\n",
    "    W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2. / hidden_size1)\n",
    "    b2 = np.zeros((1, hidden_size2))\n",
    "    W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2. / hidden_size2)\n",
    "    b3 = np.zeros((1, output_size))\n",
    "    \n",
    "    n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(X_train))\n",
    "            \n",
    "            batch_X = X_train[start_idx:end_idx]\n",
    "            batch_y = y_train[start_idx:end_idx]\n",
    "            \n",
    "            y_pred, (A1, A2) = forward_pass(batch_X, W1, b1, W2, b2, W3, b3)\n",
    "            \n",
    "            W1, b1, W2, b2, W3, b3, batch_loss = backpropagation_two_hidden_layers(\n",
    "                batch_X, batch_y, W1, b1, W2, b2, W3, b3, learning_rate\n",
    "            )\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "        \n",
    "        train_pred, _ = forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n",
    "        test_pred, _ = forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
    "        \n",
    "        train_metrics = evaluate_model(y_train, train_pred, scaler_y)\n",
    "        test_metrics = evaluate_model(y_test, test_pred, scaler_y)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Loss: {total_loss/n_batches:.4f}\")\n",
    "        print(\"Training Metrics:\")\n",
    "        for metric, value in train_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print(\"Test Metrics:\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "# Train for each splitting case\n",
    "splitting_cases = [\n",
    "    (X1_train, X1_test, \"Case 1: No new proteins in test\"),\n",
    "    (X2_train, X2_test, \"Case 2: New proteins in test\"),\n",
    "    (X3_train, X3_test, \"Case 3: No new drugs in test\"),\n",
    "    (X4_train, X4_test, \"Case 4: New drugs in test\")\n",
    "]\n",
    "\n",
    "for train_df, test_df, case_name in splitting_cases:\n",
    "    print(f\"\\nTraining for {case_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, y_train, scaler_X, scaler_y = prepare_data(train_df)\n",
    "    X_test, y_test, _, _ = prepare_data(test_df)\n",
    "    \n",
    "    # Train model\n",
    "    train_model(X_train, y_train, X_test, y_test, scaler_y,\n",
    "                input_size=X_train.shape[1],\n",
    "                epochs=50,\n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, precision_recall_curve, auc\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Data preparation (replace with actual data loading logic)\n",
    "# Example input data based on the notebook structure\n",
    "data = pd.DataFrame({\n",
    "    \"encoded_sequence\": [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]],\n",
    "    \"affinity\": [0.5, 0.8, 0.2, 0.9]\n",
    "})\n",
    "data = data.explode(\"encoded_sequence\").reset_index(drop=True)\n",
    "X = np.array(data[\"encoded_sequence\"]).reshape(-1, 1, 1, 1)  # Input reshaped for CNN\n",
    "Y = np.array(data[\"affinity\"])  # Output\n",
    "\n",
    "# Splitting cases\n",
    "def split_case_1(X, Y):\n",
    "    split_idx = int(len(X) * 0.7)\n",
    "    return X[:split_idx], X[split_idx:], Y[:split_idx], Y[split_idx:]\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    pcc, _ = pearsonr(y_true, y_pred)\n",
    "    return mse, mae, r2, pr_auc, pcc\n",
    "\n",
    "# AlexNet Model\n",
    "def build_alexnet(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "        Conv2D(256, kernel_size=(5, 5), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "        Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ResNet Model\n",
    "def build_resnet(input_shape):\n",
    "    base_model = ResNet50(include_top=False, weights=None, input_shape=input_shape, pooling='avg')\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Training Function\n",
    "def train_and_evaluate_cnn(X_train, X_test, Y_train, Y_test, model_fn, input_shape, epochs=10, batch_size=32):\n",
    "    model = model_fn(input_shape)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=1)\n",
    "\n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    mse, mae, r2, pr_auc, pcc = compute_metrics(Y_test, predictions)\n",
    "    print(f\"Final Metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, PR AUC: {pr_auc:.4f}, PCC: {pcc:.4f}\\n\")\n",
    "\n",
    "# Example usage\n",
    "X_train, X_test, Y_train, Y_test = split_case_1(X, Y)\n",
    "input_shape = (1, 1, 1)  # Replace with actual input shape\n",
    "\n",
    "print(\"Training AlexNet:\")\n",
    "train_and_evaluate_cnn(X_train, X_test, Y_train, Y_test, build_alexnet, input_shape, epochs=5)\n",
    "\n",
    "print(\"Training ResNet:\")\n",
    "train_and_evaluate_cnn(X_train, X_test, Y_train, Y_test, build_resnet, input_shape, epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2799834,
     "sourceId": 8170366,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
