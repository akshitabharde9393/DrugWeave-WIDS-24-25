{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8170366,"sourceType":"datasetVersion","datasetId":2799834}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:28:59.254857Z","iopub.execute_input":"2025-01-01T10:28:59.255328Z","iopub.status.idle":"2025-01-01T10:28:59.618942Z","shell.execute_reply.started":"2025-01-01T10:28:59.255286Z","shell.execute_reply":"2025-01-01T10:28:59.618058Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/davis-and-kiba/kiba.txt\n/kaggle/input/davis-and-kiba/davis.txt\n/kaggle/input/davis-and-kiba/davis-filter.txt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Processing the Data and encoding sequence for MLP** ","metadata":{}},{"cell_type":"code","source":"davis_filter_dir = '/kaggle/input/davis-and-kiba/davis-filter.txt'\nkiba_dir = '/kaggle/input/davis-and-kiba/kiba.txt'\ndavis_dir = '/kaggle/input/davis-and-kiba/davis.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:28:59.620172Z","iopub.execute_input":"2025-01-01T10:28:59.620650Z","iopub.status.idle":"2025-01-01T10:28:59.624498Z","shell.execute_reply.started":"2025-01-01T10:28:59.620605Z","shell.execute_reply":"2025-01-01T10:28:59.623648Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"col_name = ['drug_id', 'prot_id', 'SMILES', 'sequence amino acid', 'sequence amino acid affinity']\ndd = pd.read_csv(davis_dir, sep=' ', header=None)  \ndd.columns = col_name\ndd = pd.DataFrame(dd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:28:59.626193Z","iopub.execute_input":"2025-01-01T10:28:59.626436Z","iopub.status.idle":"2025-01-01T10:29:00.125799Z","shell.execute_reply.started":"2025-01-01T10:28:59.626414Z","shell.execute_reply":"2025-01-01T10:29:00.124891Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"ddf = pd.read_csv(davis_filter_dir, sep=' ', header=None)  # TODO modify the dataset_dir\nddf.columns = col_name\nddf = pd.DataFrame(ddf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:00.127617Z","iopub.execute_input":"2025-01-01T10:29:00.128088Z","iopub.status.idle":"2025-01-01T10:29:00.281161Z","shell.execute_reply.started":"2025-01-01T10:29:00.128045Z","shell.execute_reply":"2025-01-01T10:29:00.280401Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dk = pd.read_csv(kiba_dir, sep=' ', header=None)  # TODO modify the dataset_dir\ndk.columns = col_name\ndk = pd.DataFrame(dk)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:00.282080Z","iopub.execute_input":"2025-01-01T10:29:00.282378Z","iopub.status.idle":"2025-01-01T10:29:01.811972Z","shell.execute_reply.started":"2025-01-01T10:29:00.282342Z","shell.execute_reply":"2025-01-01T10:29:01.811016Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"Davis_shape:\", dd.shape)\nprint(\"Davis_filter_shape:\", ddf.shape)\nprint(\"kiba_shape:\", dk.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:01.813062Z","iopub.execute_input":"2025-01-01T10:29:01.813414Z","iopub.status.idle":"2025-01-01T10:29:01.819853Z","shell.execute_reply.started":"2025-01-01T10:29:01.813382Z","shell.execute_reply":"2025-01-01T10:29:01.818904Z"}},"outputs":[{"name":"stdout","text":"Davis_shape: (30056, 5)\nDavis_filter_shape: (9125, 5)\nkiba_shape: (118254, 5)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"amino_acids = \"ARNDCEQGHILKMFPSTWYV\"  # 20 standard amino acids\namino_acid_to_number = {aa: idx + 1 for idx, aa in enumerate(amino_acids)}\ndef encode_sequence(sequence):\n    return [amino_acid_to_number[aa] for aa in sequence if aa in amino_acid_to_number]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:01.820878Z","iopub.execute_input":"2025-01-01T10:29:01.821150Z","iopub.status.idle":"2025-01-01T10:29:01.834568Z","shell.execute_reply.started":"2025-01-01T10:29:01.821122Z","shell.execute_reply":"2025-01-01T10:29:01.833625Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dd['encoded_sequence'] = dd['sequence amino acid'].apply(encode_sequence)\nddf['encoded_sequence'] = ddf['sequence amino acid'].apply(encode_sequence)\ndk['encoded_sequence'] = dk['sequence amino acid'].apply(encode_sequence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:01.835574Z","iopub.execute_input":"2025-01-01T10:29:01.835875Z","iopub.status.idle":"2025-01-01T10:29:14.879104Z","shell.execute_reply.started":"2025-01-01T10:29:01.835853Z","shell.execute_reply":"2025-01-01T10:29:14.877990Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# **Evaluation Metrics**","metadata":{}},{"cell_type":"code","source":"def h(m):\n    if m > 0:\n        return 1\n    elif m == 0:\n        return 0.5\n    else:\n        return 0\n\ndef ConcordanceIndex(Affinities,Predictions):\n    Z = 0 \n    concordant_pairs = 0\n    \n    n = len(Affinities)\n    for i in range(n):\n        for j in range(i + 1, n):\n            if Affinities[i] > Affinities[j]:\n                Z += 1\n                concordant_pairs += h(Predictions[i] - Predictions[j])\n    \n    if Z > 0:\n        return concordant_pairs / Z\n    else: \n        return 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:56:30.598574Z","iopub.execute_input":"2025-01-01T10:56:30.599097Z","iopub.status.idle":"2025-01-01T10:56:30.606998Z","shell.execute_reply.started":"2025-01-01T10:56:30.599039Z","shell.execute_reply":"2025-01-01T10:56:30.605762Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def mean_squared_error(Affinities,Predictions):\n    errors = np.array(Affinities) - np.array(Predictions)\n    return np.mean(errors**2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:14.891887Z","iopub.execute_input":"2025-01-01T10:29:14.892294Z","iopub.status.idle":"2025-01-01T10:29:14.909201Z","shell.execute_reply.started":"2025-01-01T10:29:14.892255Z","shell.execute_reply":"2025-01-01T10:29:14.908250Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ndef R2(Affinities, Predictions):\n    Affinities = np.array(Affinities)\n    Predictions = np.array(Predictions)\n    \n    SST = np.sum((Affinities - np.mean(Affinities))**2)\n    SSR = np.sum((Affinities - Predictions)**2)\n    \n    return 1 - (SSR/SST)\n\ndef R02(Affinities, Predictions):\n    Affinities = np.array(Affinities).reshape(-1, 1)\n    Predictions = np.array(Predictions).reshape(-1, 1)\n    \n    model = LinearRegression(fit_intercept=False)\n    model.fit(Predictions, Affinities)\n    Predictions = model.predict(Predictions)\n    \n    SST = np.sum((Affinities - np.mean(true_values))**2)\n    SSR = np.sum((Affinities - predictions)**2)\n    \n    return 1 - (SSR/SST)\n\ndef RMSsquared(Affinities, Predictions):\n    Rsquared = R2(Affinities, Predictions)\n    R0squared = R02(Affinities, Predictions)\n    \n    return Rsquared * (1 - np.sqrt(Rsquared - R0squared))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:14.910288Z","iopub.execute_input":"2025-01-01T10:29:14.910573Z","iopub.status.idle":"2025-01-01T10:29:16.260839Z","shell.execute_reply.started":"2025-01-01T10:29:14.910548Z","shell.execute_reply":"2025-01-01T10:29:16.260022Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def Pearson_correlation(Affinities, Predictions):\n    Affinities = np.array(Affinities)\n    Predictions = np.array(Predictions)\n    \n    TrueAvg = np.mean(Affinities)\n    PredAvg = np.mean(Predictions)\n    \n    Num = np.sum((Affinities - TrueAvg) * (Predictions - PredAvg))\n    Den = np.sqrt(np.sum((Affinities - TrueAvg)**2) * np.sum((Predictions - PredAvg)**2))\n    \n    return Num/Den","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:16.261824Z","iopub.execute_input":"2025-01-01T10:29:16.262222Z","iopub.status.idle":"2025-01-01T10:29:16.267507Z","shell.execute_reply.started":"2025-01-01T10:29:16.262196Z","shell.execute_reply":"2025-01-01T10:29:16.266614Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, auc\n\ndef compute_aupr(Affinities, Predictions):\n    precision, recall, _ = precision_recall_curve(Affinities, Predictions)\n    return auc(recall, precision)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:16.268424Z","iopub.execute_input":"2025-01-01T10:29:16.268829Z","iopub.status.idle":"2025-01-01T10:29:16.286764Z","shell.execute_reply.started":"2025-01-01T10:29:16.268791Z","shell.execute_reply":"2025-01-01T10:29:16.285621Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef backpropagation_two_hidden_layers(X, y, W1, b1, W2, b2, W3, b3, learning_rate):\n    \n    Z1 = np.dot(X, W1) + b1\n    A1 = sigmoid(Z1)\n    \n    Z2 = np.dot(A1, W2) + b2\n    A2 = sigmoid(Z2)\n    \n    Z3 = np.dot(A2, W3) + b3\n    y_pred = Z3 \n    \n    n = y.shape[0]\n    loss = np.mean((y - y_pred)**2)\n    dLoss_dZ3 = (2 / n) * (y_pred - y)\n    dLoss_dW3 = np.dot(A2.T, dLoss_dZ3)\n    dLoss_db3 = np.sum(dLoss_dZ3, axis=0, keepdims=True)\n    \n    dLoss_dA2 = np.dot(dLoss_dZ3, W3.T)\n    dLoss_dZ2 = dLoss_dA2 * sigmoid_derivative(Z2)\n    dLoss_dW2 = np.dot(A1.T, dLoss_dZ2)\n    dLoss_db2 = np.sum(dLoss_dZ2, axis=0, keepdims=True)\n    \n    dLoss_dA1 = np.dot(dLoss_dZ2, W2.T)\n    dLoss_dZ1 = dLoss_dA1 * sigmoid_derivative(Z1)\n    dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n    dLoss_db1 = np.sum(dLoss_dZ1, axis=0, keepdims=True)\n    \n    W1 -= learning_rate * dLoss_dW1\n    b1 -= learning_rate * dLoss_db1\n    W2 -= learning_rate * dLoss_dW2\n    b2 -= learning_rate * dLoss_db2\n    W3 -= learning_rate * dLoss_dW3\n    b3 -= learning_rate * dLoss_db3\n    \n    return W1, b1, W2, b2, W3, b3, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:55:03.544140Z","iopub.execute_input":"2025-01-01T10:55:03.544497Z","iopub.status.idle":"2025-01-01T10:55:03.552661Z","shell.execute_reply.started":"2025-01-01T10:55:03.544461Z","shell.execute_reply":"2025-01-01T10:55:03.551750Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# **Data Training on Davis Dataset**","metadata":{}},{"cell_type":"code","source":"dd.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:29:16.304898Z","iopub.execute_input":"2025-01-01T10:29:16.305240Z","iopub.status.idle":"2025-01-01T10:29:16.338084Z","shell.execute_reply.started":"2025-01-01T10:29:16.305202Z","shell.execute_reply":"2025-01-01T10:29:16.336829Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"    drug_id       prot_id                                             SMILES  \\\n0  11314340          AAK1  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n1  11314340   ABL1(E255K)  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n2  11314340   ABL1(F317I)  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n3  11314340  ABL1(F317I)p  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n4  11314340   ABL1(F317L)  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n\n                                 sequence amino acid  \\\n0  MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...   \n1  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n2  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n3  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n4  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n\n   sequence amino acid affinity  \\\n0                      7.366532   \n1                      5.000000   \n2                      5.000000   \n3                      5.000000   \n4                      5.000000   \n\n                                    encoded_sequence  \n0  [13, 12, 12, 14, 14, 4, 16, 2, 2, 6, 7, 8, 8, ...  \n1  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  \n2  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  \n3  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  \n4  [15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>drug_id</th>\n      <th>prot_id</th>\n      <th>SMILES</th>\n      <th>sequence amino acid</th>\n      <th>sequence amino acid affinity</th>\n      <th>encoded_sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11314340</td>\n      <td>AAK1</td>\n      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n      <td>MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...</td>\n      <td>7.366532</td>\n      <td>[13, 12, 12, 14, 14, 4, 16, 2, 2, 6, 7, 8, 8, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11314340</td>\n      <td>ABL1(E255K)</td>\n      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n      <td>5.000000</td>\n      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11314340</td>\n      <td>ABL1(F317I)</td>\n      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n      <td>5.000000</td>\n      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11314340</td>\n      <td>ABL1(F317I)p</td>\n      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n      <td>5.000000</td>\n      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11314340</td>\n      <td>ABL1(F317L)</td>\n      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n      <td>5.000000</td>\n      <td>[15, 14, 18, 12, 10, 11, 3, 15, 11, 11, 6, 2, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"def split_case_1(df):\n    train_data = []\n    test_data = []\n    \n    for protein, group in df.groupby('prot_id'):\n        n_train = int(len(group) * 0.7)  # 70% for train\n        group = group.sample(frac=1, random_state=42)  # Shuffle the data\n        train_data.append(group.iloc[:n_train])\n        test_data.append(group.iloc[n_train:])\n    \n    train = pd.concat(train_data).reset_index(drop=True)\n    test = pd.concat(test_data).reset_index(drop=True)\n    return train, test\n\ndef split_case_2(df):\n    unique_proteins = df['prot_id'].unique()\n    n_train_proteins = int(len(unique_proteins) * 0.9)  # 90% of proteins for train\n    train_proteins = np.random.choice(unique_proteins, size=n_train_proteins, replace=False)\n    \n    train = df[df['prot_id'].isin(train_proteins)]\n    test = df[~df['prot_id'].isin(train_proteins)]\n    return train, test\n\ndef split_case_3(df):\n    train_data = []\n    test_data = []\n    \n    # Group by drug and split each group\n    for drug, group in df.groupby('drug_id'):\n        n_train = int(len(group) * 0.7)  # 70% for train\n        group = group.sample(frac=1, random_state=42)  # Shuffle the data\n        train_data.append(group.iloc[:n_train])\n        test_data.append(group.iloc[n_train:])\n    \n    train = pd.concat(train_data).reset_index(drop=True)\n    test = pd.concat(test_data).reset_index(drop=True)\n    return train, test\n\ndef split_case_4(df):\n    unique_drugs = df['drug_id'].unique()\n    n_train_drugs = int(len(unique_drugs) * 0.9)  # 90% of drugs for train\n    train_drugs = np.random.choice(unique_drugs, size=n_train_drugs, replace=False)\n    \n    train = df[df['drug_id'].isin(train_drugs)]\n    test = df[~df['drug_id'].isin(train_drugs)]\n    return train, test\n\n# Generate splits\nX1_train, X1_test = split_case_1(dd)  # Case 1: No new proteins in test\nX2_train, X2_test = split_case_2(dd)  # Case 2: New proteins in test\nX3_train, X3_test = split_case_3(dd)  # Case 3: No new drugs in test\nX4_train, X4_test = split_case_4(dd)  # Case 4: New drugs in test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:36:41.212238Z","iopub.execute_input":"2025-01-01T10:36:41.212663Z","iopub.status.idle":"2025-01-01T10:36:41.644047Z","shell.execute_reply.started":"2025-01-01T10:36:41.212616Z","shell.execute_reply":"2025-01-01T10:36:41.642952Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\n\ndef pad_sequences(sequences, max_length=None):\n    if max_length is None:\n        max_length = max(len(seq) for seq in sequences)\n    return [seq + [0] * (max_length - len(seq)) for seq in sequences]\n\ndef prepare_data(df):\n    # Pad sequences to same length\n    sequences = pad_sequences(df['encoded_sequence'].values)\n    X = np.array(sequences)\n    y = df['sequence amino acid affinity'].values.reshape(-1, 1)\n    \n    # Normalize features\n    scaler_X = StandardScaler()\n    scaler_y = StandardScaler()\n    \n    X = scaler_X.fit_transform(X)\n    y = scaler_y.fit_transform(y)\n    \n    return X, y, scaler_X, scaler_y\n\ndef forward_pass(X, W1, b1, W2, b2, W3, b3):\n    Z1 = np.dot(X, W1) + b1\n    A1 = sigmoid(Z1)\n    \n    Z2 = np.dot(A1, W2) + b2\n    A2 = sigmoid(Z2)\n    \n    Z3 = np.dot(A2, W3) + b3\n    y_pred = Z3\n    \n    return y_pred, (A1, A2)\n\ndef evaluate_model(y_true, y_pred, scaler_y):\n    y_true_orig = scaler_y.inverse_transform(y_true)\n    y_pred_orig = scaler_y.inverse_transform(y_pred)\n    \n    # Calculate metrics\n    mse = mean_squared_error(y_true_orig, y_pred_orig)\n    pearson = Pearson_correlation(y_true_orig.flatten(), y_pred_orig.flatten())\n    ci = ConcordanceIndex(y_true_orig.flatten(), y_pred_orig.flatten())\n    r2 = R2(y_true_orig.flatten(), y_pred_orig.flatten())\n    \n    return {\n        'MSE': mse,\n        'Pearson': pearson,\n        'CI': ci,\n        'R2': r2\n    }\n\ndef train_model(X_train, y_train, X_test, y_test, scaler_y, input_size=1000, hidden_size1=512, hidden_size2=256, output_size=1, \n                learning_rate=0.001, epochs=100, batch_size=32):\n    # Initialize weights\n    W1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2. / input_size)\n    b1 = np.zeros((1, hidden_size1))\n    W2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2. / hidden_size1)\n    b2 = np.zeros((1, hidden_size2))\n    W3 = np.random.randn(hidden_size2, output_size) * np.sqrt(2. / hidden_size2)\n    b3 = np.zeros((1, output_size))\n    \n    n_batches = int(np.ceil(len(X_train) / batch_size))\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        \n        indices = np.random.permutation(len(X_train))\n        X_train = X_train[indices]\n        y_train = y_train[indices]\n        \n        for i in range(n_batches):\n            start_idx = i * batch_size\n            end_idx = min(start_idx + batch_size, len(X_train))\n            \n            batch_X = X_train[start_idx:end_idx]\n            batch_y = y_train[start_idx:end_idx]\n            \n            y_pred, (A1, A2) = forward_pass(batch_X, W1, b1, W2, b2, W3, b3)\n            \n            W1, b1, W2, b2, W3, b3, batch_loss = backpropagation_two_hidden_layers(\n                batch_X, batch_y, W1, b1, W2, b2, W3, b3, learning_rate\n            )\n            \n            total_loss += batch_loss\n        \n        train_pred, _ = forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n        test_pred, _ = forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n        \n        train_metrics = evaluate_model(y_train, train_pred, scaler_y)\n        test_metrics = evaluate_model(y_test, test_pred, scaler_y)\n        \n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        print(f\"Loss: {total_loss/n_batches:.4f}\")\n        print(\"Training Metrics:\")\n        for metric, value in train_metrics.items():\n            print(f\"{metric}: {value:.4f}\")\n        print(\"Test Metrics:\")\n        for metric, value in test_metrics.items():\n            print(f\"{metric}: {value:.4f}\")\n    \n    return W1, b1, W2, b2, W3, b3\n\n# Train for each splitting case\nsplitting_cases = [\n    (X1_train, X1_test, \"Case 1: No new proteins in test\"),\n    (X2_train, X2_test, \"Case 2: New proteins in test\"),\n    (X3_train, X3_test, \"Case 3: No new drugs in test\"),\n    (X4_train, X4_test, \"Case 4: New drugs in test\")\n]\n\nfor train_df, test_df, case_name in splitting_cases:\n    print(f\"\\nTraining for {case_name}\")\n    print(\"-\" * 50)\n    \n    # Prepare data\n    X_train, y_train, scaler_X, scaler_y = prepare_data(train_df)\n    X_test, y_test, _, _ = prepare_data(test_df)\n    \n    # Train model\n    train_model(X_train, y_train, X_test, y_test, scaler_y,\n                input_size=X_train.shape[1],\n                epochs=50,\n                batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:57:10.055372Z","iopub.execute_input":"2025-01-01T10:57:10.055775Z"}},"outputs":[{"name":"stdout","text":"\nTraining for Case 1: No new proteins in test\n--------------------------------------------------\n\nEpoch 1/50\nLoss: 0.9911\nTraining Metrics:\nMSE: 0.7647\nPearson: 0.1900\nCI: 0.5810\nR2: 0.0274\nTest Metrics:\nMSE: 0.7535\nPearson: 0.2373\nCI: 0.6267\nR2: 0.0416\n\nEpoch 2/50\nLoss: 0.9631\nTraining Metrics:\nMSE: 0.7531\nPearson: 0.2171\nCI: 0.5969\nR2: 0.0422\nTest Metrics:\nMSE: 0.7384\nPearson: 0.2702\nCI: 0.6416\nR2: 0.0608\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}